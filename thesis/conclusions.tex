\chapter{Conclusions}

We presented a general model of code-breaking games based on propositional logic,
  which can fit Mastermind, the counterfeit coin problem and many others.
Experiment equivalence was introduced and we proved that
  equivalent experiments can be disregarded during strategy analysis and
  optimal strategy synthesis.
We suggested an algorithm for equivalence
  testing based on graph isomorphism.

A computer language for code-breaking game specification was introduced
  and we developed a computer program that can perform various tasks
  with a given code-breaking game.
Using the tool, we reproduced some of the existing results for Mastermind,
  analysed other code-breaking games and
  evaluated strategies for experiment selection
  based on the number of fixed variables.

There are many more interesting things to try in this framework.
We present a few suggestions for future work in the next paragraphs.

First, our code-breaking game model can be further generalized in many ways.
Numerous possibilities arise if we allow experiments to have different costs.

Imagine Mastermind with another type of experiment
  that directly tells you a colour at a specified position.
What price must the new experiment have so that
  it is worth using it, given that the standard guess has unit cost?
%Another possibility is to limit the number of experiments of a type.

% Similarly, it may be interesting to limit
%   the number of experiments of a type.
% In the previous case, how would you play if you can
%   perform the new experiment only once?

Second, one-step look-ahead strategies provide us with
  a simple heuristics to select experiments.
However, if some experiments are assigned the same value, the
  lexicographically smaller experiment is chosen,
  which is not very reasonable.
% In fact, the ``parts'' strategy for Mastermind with 4 pegs and 6 colours
%   performs so well only due to a ``lucky'' choice in the first step.
% Experiments AABC and ABCD have both 14 satisfiable outcome
%   but the strategy chooses AABC because as it is lexicographically smaller.
\emph{Randomized strategies},
  where the experiment is selected from the experiments
  with the best value with uniform distribution
  may lead to many interesting results.

Third, look-ahead strategies can be naturally extended to more than one step.
This would lead to the \emph{minimax algorithm} applied to the tree of
  possible outcomes and experiments in the next rounds.
Evaluation of such strategies would be much more computationally demanding.
Would their performance be significantly better?

Finally, several completely different approaches for strategy synthesis
  were suggested for Mastermind.
In particular, genetic algorithms proved to be very useful for problems
  of larger sizes as they scale much better than the backtracking approach.
Can we apply these methods in our model?

For now, these interesting questions remain open.
We hope they will lead to further research in this area.
