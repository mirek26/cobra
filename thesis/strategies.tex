\section{Strategies in general}

This section introduces the concept of a strategy for experiment selection.
We define the worst-case and the average-case number of experiments of a strategy
 and optimal strategies. Further, we examine several strategy classes.

\begin{definition}[Strategy]\label{def:strategy}
A \emph{strategy} is a function $\stg: \Omega^* -> \Exp$,
  determining the next experiment for a given finite solving process.
\end{definition}

A strategy $\stg$ together with a valuation $\val\in\Vals$
  induce an infinite solving process
  \[
  \procstg{\stg}{\val} = (\exp_1, \form_1), (\exp_2, \form_2), ...,
  \]
  where
  $\exp_{i+1} = \stg(\procstg{\stg}{\val}[1:i])$
  and
  $\form_{i+1}$ is the formula from $\outcome(\exp_{i+1})$
  satisfied by $\val$,
  for all $i\in\Nset$.
Due to the well-formed property of the game,
  there is exactly one such $\form_{i+1}$.

We define the \emph{length} of a strategy $\stg$ on a valuation $\val$,
  denoted $\stglen{\stg}{\val}$,
  as the smallest $k\in\Nseto$ such that
  $\stgknow{\stg}{\val}{k}$ uniquely determines the code, i.e.
  \[
  \stglen{\stg}{\val} = \min \;\{ k\in\Nseto \| \numval{\stgknow{\stg}{\val}{k}} = 1 \}
  \]


The \emph{worst-case number of experiments} $\lenmax{\stg}$
  of a strategy $\stg$ is the maximal length of the strategy on a valuation $\val$,
  over all $\val\in\Vals$, i.e.
  \[
  \lenmax{\stg} = \max_{\val\in\Vals} \stglen{\stg}{\val}.
  \]

The \emph{average-case number of experiments} $\lenexp{\stg}$
  of a strategy $\stg$ is the expected number of experiments if the code
  is selected from models of $\init$ with uniform distribution, i.e.
  \[
  \lenexp{\stg} = \frac{
    \sum_{\val\in\Vals} \stglen{\stg}{\val}
    }{\numval{\init}}.
  \]

We say that a strategy $\stg$ \emph{solves the game} if $\lenmax{\stg}$ is finite.
Note that $\lenmax{\stg}$ is finite if and only if $\lenexp{\stg}$ is finite.
The game is \emph{solvable} if there exists a strategy that solves the game.

\medskip

\begin{definition}[Optimal strategy]
A strategy $\stg$ is \emph{worst-case optimal} if
  $\lenmax{\stg} <= \lenmax{\stg'}$ for any strategy $\stg'$.
A strategy $\stg$ is \emph{average-case optimal} if
  $\lenexp{\stg} <= \lenexp{\stg'}$ for any strategy $\stg'$.
\end{definition}

The following lemma provides us with a lower bound on the number of
experiments of a worst-case optimal strategy.
The next lemma presents an important observation about the induced solving processes,
  which is needed in several upcoming theorems.

\begin{lemma} \label{lma:lbound}
Let $b = \max_{\expt\in\Expt} |\outcome(\expt)|$ be the maximal number of
  possible outcomes of an experiment. Then for every strategy $\stg$,
  \[
  \lenmax{\stg} >= \lceil \log_b(\numval{\init}) \rceil.
  \]
\end{lemma}

\begin{proof}
Let us fix a strategy $\stg$ and $k = \lenmax{\stg}$.
For an unknown model $\val$ of $\init$,
  $\stgknow{\stg}{\val}{k}$ can take up to
  $b^k$ different values.
By pigeon-hole principle, if $\numval{\init} > b^k$, there must be a valuation
  $v$ such that $\numval{\stgknow{\stg}{\val}{k}} > 1$.
This would be a contradiction with $k = \lenmax{\stg}$ and, therefore,
  $\numval{\init} <= b^k$, which is equivalent with the statement of the lemma.
  \qed
\end{proof}

\begin{lemma} \label{lma:accumulatedknowledge}
Let $\stg$ be a strategy and let $\val_1, \val_2 \in\Vals$.
If $\val_1$ is a model of $\stgknow{\stg}{\val_2}{k}$,
  then $\procstg{\stg}{\val_1}[1:k] = \procstg{\stg}{\val_2}[1:k]$.
\end{lemma}

\begin{proof}
Let $\proc_1 = \procstg{\stg}{\val_1}$, $\proc_2 = \procstg{\stg}{\val_2}$
and consider the first place where $\proc_1$ and $\proc_2$ differ.
It cannot be the $i$-th experiment as both $\proc_1(i)$ and $\proc_2(i)$
  are values of the same strategy on the same solving process:
$\proc_1(i) = \stg(\proc_1[1:i-1]) =
              \stg(\proc_2[1:i-1]) = \proc_2(i)$.

Therefore, it must be an outcome of the $i$-th experiment,
  i.e. $\proc_1[i] \not= \proc_2[i]$
  for some $i <= k$.
Since $\val_1$ satisfies $\aknow{\proc_2}{k}$ and $i <= k$,
  it satisfies $\proc_2[i]$ as well.
However, $\val_1$ always satisfies $\proc_1[i]$ and
  both $\proc_1[i]$ and $\proc_2[i]$ are from the set
  $\outcome(\proc_1(i)) = \outcome(\proc_2(i))$.
Since there is exactly one satisfied experiment for each valuation in the set,
  $\proc_1[i]$ and $\proc_2[i]$ must be the same, which is
  a contradiction. \qed
\end{proof}

\begin{example} \label{ex:run2}
Recall our running example of the counterfeit coin problem with four coins,
introduced in \autoref{ex:run1}.

Consider a strategy $\stg$ defined as follows.
For simplicity, we denote experiments by their parametrizations
  (this is sufficient, because the parametrized experiments have different number of parameters)
  and the outcomes by symbols $<$, $>$ and $=$,
  instead of the corresponding formula.
\[
\stg(\proc) = \left\{\begin{array}{ll}
13 & \textrm{ if } \proc = (12, <), \\
23 & \textrm{ if } \proc = (12, >),\\
14 & \textrm{ if } \proc = (12, =), (12, =), \\
34 & \textrm{ if } \proc = (12, =), (12, =), (14, =), \\
12 & \textrm{ otherwise.}
\end{array}\right.
\]

Let $\val\in\Vals$ be a valuation such that $v(x_3) = v(y) = 1$.
The solving process induced by $\stg$ on $v$ is
\[
\procstg{\stg}{\val} = (12, =), (12, =), (14, =), (34, >), (12, =), (12, =), ...
\]
The length of $\stg$ on $\val$ is 4, because $v$ is the only model of
  the accumulated knowledge after 4 experiments,
\[
\exactly_1(x_1,x_2,x_3,x_4) \wedge \neg(x_1 \vee x_2) \wedge \neg(x_1 \vee x_2)
\wedge \neg(x_1 \vee x_4) \wedge ((x_3 \wedge y) \vee (x_4 \wedge \neg y)).
\]

The strategy pointlessly repeats the experiment $12$
  if the outcome in the first step is $=$.
In fact, every valuation is revealed by $\stg$ in at most 4 experiments,
  which means that $\lenmax{\stg} = 4$.

\autoref{lma:lbound} gives us the lower bound $ \lceil\log_3(8)\rceil = 2$
 on the worst-case number of experiments of an optimal strategy.
However, we already know from \autoref{th:coins12} that the minimal number
  of experiments needed to reveal the code in the worst-case is 3. \eqed
\end{example}

\subsection{Non-adaptive strategies}

Non-adaptive strategies correspond to the well-studied problems of
  static Mastermind and
  non-adaptive strategies for
  the counterfeit coin problem\cite{mm-static}\cite{coins-nonadaptive}.
We define them here only to show the possibility of formulating the
  corresponding problems
  in our framework but we do not study them any further.

\begin{definition}[Non-adaptive strategy]
A strategy $\stg$ is \emph{non-adaptive} if it decides the next experiment
  based on the length of the solving process only, i.e.
  whenever $\proc_1$ and $\proc_2$ are processes such that
  $|\proc_1| = |\proc_2|$,
  then
  $\stg(\proc_1) = \stg(\proc_2)$.

Non-adaptive strategies can be considered functions $\stgx: \Nseto -> \Exp$,
where $\tau(|\proc|) = \stg(\proc)$.
\end{definition}

\subsection{Memory-less strategies}

According to the general definition,
  a strategy can decide the next experiment on the basis of the exact history
  of the solving process.
In can be argued that the accumulated knowledge should be sufficient for
  the decision as the overall nature of code-breaking games is memory-less
  and the course of a game depends only on the accumulated knowledge.
Here we define memory-less strategies and prove that it is indeed the case.

\begin{definition}[Memory-less strategy]
A strategy $\stg$ is \emph{memory-less} if it decides the next experiment
  based on the accumulated knowledge only, i.e.
  whenever $\proc_1$ and $\proc_2$ are processes such that if
  $\tknow{\proc_1} \equiv \tknow{\proc_2}$
  then
  $\stg(\proc_1) = \stg(\proc_2)$.

Memory-less strategies can be considered functions
  $\stgx: \Formr -> \Exp$ such that
  $\form_1\equiv\form_2 ==> \stgx(\form_1)=\stgx(\form_2)$.
Then $\stg(\proc) = \stgx(\tknow{\proc})$.
\end{definition}

Note that the number of non-equivalent formulas over variable $\Var$
  is finite and, therefore, the number of memory-less strategies for a fixed
  code-breaking game is finite as well.

Now we prove some basic properties of memory-less strategies.
The following lemma says that once we do not get any new information
  from the experiment selected by a experiment,
  we never get any new information with the strategy.
Then, the theorem below proves that there exists an optimal
  memory-less strategy.

\begin{lemma}
Let $\stg$ be a memory-less strategy and $\val\in\Vals$.
If there exists $k\in\Nset$ such that
  $\numval{\stgknow{\stg}{\val}{k}} = \numval{\stgknow{\stg}{\val}{k+1}}$,
 then
  $\numval{\stgknow{\stg}{\val}{k}} = \numval{\stgknow{\stg}{\val}{k+l}}$
 for any $l\in\Nset$.
\end{lemma}

\begin{proof}
To simplify the notation, let $\know^k = \stgknow{\stg}{\val}{k}$ be the accumulated knowledge after $k$ experiments.
Since every model of $\know^{k+1}$ is also a model of $\know^k$ and
  $\numval{\know^{k}} = \numval{\know^{k+1}}$, the sets of
  models of $\know^{k}$ and $\know^{k+1}$ are exactly the same
  and the formulas are thus equivalent.
This implies $\stg(\know^{k}) = \stg(\know^{k+1})$
  and $\know^{k+2} \equiv \know^{k+1}$.
By induction,
  $\stg(\know^{k+l}) = \stg(\know^{k})$ and
  $\know^{k+l} \equiv \know^{k}$
  for any $l\in\Nset$.\qed
\end{proof}

\begin{theorem} \label{th:memless}
Let $\stg$ be a strategy.
Then there exists a memory-less strategy $\stgx$ such that
  $\stglen{\stg}{\val} >= \stglen{\stgx}{\val}$ for all $\val\in\Vals$.
\end{theorem}

\begin{proof}
Let us show the exact construction of $\stgx$ from $\stg$.
First, we order the formulas of $\Formr$ by their number of models
  from the least. Let $\form_i$ be the $i$-th formula in this order.
We build a sequence of strategies $\stg_0, \stg_1, \stg_2, ...$
  inductively in the following way.
Let $\stg_0 = \stg$.
\begin{itemize}
\item If there is no $v\in\Vals$, $k\in\Nseto$ such that
  $\stgknow{\stg_{i-1}}{v}{k} \equiv \form_i$, select any $\exp\in\Exp$ and
  define $\stg_i$ by
\[
\stg_i(\proc) = \left\{
 \begin{array}{lll}
 \stg_{i-1}(\proc)  & \textrm{ if } \tknow{\proc}\not\equiv\form_i,\\
 \exp               & \textrm{ if } \tknow{\proc}\equiv\form_i.
 \end{array}
 \right.
\]
Clearly, the induced solving processes for $\stg_i$ and $\stg_{i-1}$ are the same.

\item If there exists $v\in\Vals, k\in\Nseto$ such that
  $\stgknow{\stg_{i-1}}{v}{k} \equiv \form_i$, choose the largest $l$ such that
  $\stgknow{\stg_{i-1}}{v}{l} \equiv \form_i$ and define
\[
\stg_i(\proc) = \left\{
 \begin{array}{lll}
 \stg_{i-1}(\proc)            & \textrm{ if } \tknow{\proc}\not\equiv\form_i,\\
 \procstg{\stg_{i-1}}{v}(l)   & \textrm{ if } \tknow{\proc}\equiv\form_i.
 \end{array}
 \right.
\]
First we prove that this definition is correct.
Let $v_1, v_2, k_1, k_2$ be such that
  $\stgknow{\stg_{i-1}}{v_1}{k_1}\equiv\form_i\equiv\stgknow{\stg_{i-1}}{v_2}{k_2}$
and let $l_1, l_2$ be the largest numbers such that
  $\stgknow{\stg_{i-1}}{v_1}{l_1}\equiv\form_i\equiv\stgknow{\stg_{i-1}}{v_2}{l_2}$.
Since $v_1$ satisfies $\stgknow{\stg_{i-1}}{v_2}{l_2}\equiv\form_i$,
  $\procstg{\stg_{i-1}}{v_2}[1:l_2] = \procstg{\stg_{i-1}}{v_1}[1:l_2]$
  by \autoref{lma:accumulatedknowledge}.
The same holds for $l_1$ which means that $l_1 = l_2$ and
  $\procstg{\stg_{i-1}}{v_1}(l_1) = \procstg{\stg_{i-1}}{v_1}(l_2)$, which
  proves that the definition of $\stg_i$ is independent of the exact choices
  of $v$ and $k$.

Now $\stglen{\stg_i}{v} = \stglen{\stg_{i-1}}{v} - (l-k)$, where
  $k$ is the smallest number such that
  $\stgknow{\stg_{i-1}}{v}{k}\equiv\form_i$ and
  $l$ is the largest number such that
  $\stgknow{\stg_{i-1}}{v}{l}\equiv\form_i $,
  because
  $\procstg{\stg_{i-1}}{v}(l) = \procstg{\stg_{i}}{v}(k)$ and due to the order of the formulas,
  the rest of the process is already independent of the beginning.
\end{itemize}

The last strategy of the sequence is clearly memory-less and satisfies the
  condition in the lemma. \qed
\end{proof}

\begin{corollary}
There exists a worst-case optimal strategy that is memory-less
and there exists an average-case optimal strategy that is memory-less.
\end{corollary}

\begin{example}
Recall the game and the strategy $\stg$ from \autoref{ex:run2}.
The strategy is clearly not non-adaptive, as
  $\stg((12, <)) \not= \stg((12, >))$.
It is neither memory-less as
  $\stg((12, =)) \not= \stg((12,=),(12,=))$ but
  the accumulated knowledge of the solving processes is the same.

Consider a non-adaptive strategy
 $\stgx:\; 1 \mapsto 12,\; 2\mapsto 13,\; 3\mapsto 14$.
If the counterfeit coin is among the first three coins,
  it is discovered by the strategy in two experiments.
If the counterfeit coin is the fourth coin, it requires three experiments.
Hence $\lenmax{\stgx} = 3$ and the value of $\stgx$ on
  greater numbers is irrelevant.

If we apply the construction in \autoref{th:memless} on $\stg$,
we get a memory-less strategy $\stg'$, given by

\[
\stg'(\form) = \left\{\begin{array}{ll}
13 & \textrm{ if } \form \equiv \init \wedge (x_1 \wedge \neg y) \vee (x_2 \wedge y), \\
23 & \textrm{ if } \form \equiv \init \wedge (x_1 \wedge y) \vee (x_2 \wedge \neg y),\\
14 & \textrm{ if } \form \equiv \init \wedge \neg x_1 \wedge \neg x_2, \\
34 & \textrm{ if } \form \equiv \init \wedge \neg x_1 \wedge \neg x_2 \wedge \neg x_4, \\
12 & \textrm{ otherwise.}
\end{array}\right.
\]

Notice that the valuation $v$ with $v(x_3) = v(y) = 1$ is now discovered in
  3 experiments as the strategy does not repeat the experiment 12.
Therefore, $\lenmax{\stg'} = 3$.

Both strategies $\tau$ and $\stg'$ are worst-case optimal. \eqed
\end{example}

\section{One-step look-ahead strategies} \label{sec:oslas}

Specification of a strategy in general can be very complicated.
In this section, we study a subclass of memory-less strategies that we call
  \emph{one-step look-ahead}.
These strategies select an experiment that
  minimizes the value of a given function
  on the set of possible knowledge in the next step.

\newcommand{\formset}{\Psi}
\begin{definition}[One-step look-ahead strategy]\label{def:oslas}
Let $f$ be a function of type $2^{\Formr} -> \Rset$ and
  $\eord$ a total order of experiments.
A one-step look-ahead strategy with respect to $f$ and $\eord$ and is
  a memory-less strategy $\stg$, such that
  $\stg(\form) = e$ is the minimal element of $(E,\eord)$ satisfying
\[
\forall e'\in\Exp. \;\; f(\{\:\form \wedge \formx \| \formx\in\outcome(e) \:\}) <=
  f(\{\: \form \wedge \formx \| \formx\in\outcome(e') \:\}).
\]
\end{definition}

We refer to the value $f(\{\form \wedge \formx \| \formx\in\outcome(e)\})$
  as the value of the function $f$ on the experiment $e$.
The purpose of $\eord$ is to resolve the cases in which
  the value of the function is the same on more experiments.

Several one-step look-ahead strategies for Mastermind
  have been already introduced in \autoref{sec:mm}.
In the following, we define them formally for the general model of code-breaking games.
Unless otherwise stated, the total order $\eord$ is
  the lexicographical order of the experiments.

\begin{description}
\item[Max-models.]
This strategy minimizes the worst-case number of remaining codes.
For Mastermind, this was suggested by Knuth\cite{mm-knuth}.
\[
f(\formset) = \max_{\form\in\formset} \numval{\form}.
\]

\item[Exp-models.]
This strategy minimizes the expected number of remaining codes.
  For Mastermind, this was suggested by Irwing\cite{mm-expnum}.
\[
f(\formset) = \frac{\sum_{\form\in\formset}(\numval{\form})^2}{\sum_{\form\in\formset} \numval{\form}}.
\]

\item[Ent-models.]
This strategy maximizes the entropy of the numbers of remaining codes.
For Mastermind, this was suggested by Neuwirth\cite{mm-entropy}.
\[
f(\formset) = \sum_{\form\in\formset} \frac{\numval{\form}}{N} \cdot \log \frac{\numval{\form}}{N}
  \textrm{, where } N = \sum_{\form\in\formset}\numval{\form}.
\]

\item[Parts.]
This strategy maximizes the number of satisfiable outcomes.
For Mastermind, this was suggested by Kooi\cite{mm-mostparts}.
\[
f(\formset) = - \:|\{ \form \| \form\in\formset, \;\SAT{\form} \}|.
\]
\end{description}

\newcommand{\fixed}{\#_\textrm{fixed}\:}
We suggest and analyse two more one-step look ahead strategies that are based on
the number of fixed variables of the formulas.
Let
\[
\fixed\form = |\{ x\in\Var \| \forall v.v(\form)=1 ==> v(x)=1 \}
                  \cup\{ x\in\Var \| \forall v.v(\form)=1 ==> v(x)=0 \}|
\]
be the number of fixed variables of $\form$.
Note that while the strategies above does not depend on the exact
  formalization of a problem, the number of fixed variables may differ for
  different encodings.
For example, recall the two possible formalisations of the counterfeit coin problem defined in \autoref{ex:cc1}.
The numbers of the fixed variables in the outcome formulas differ, which
  means that the strategy may select different experiments.

\begin{description}
\item[Min-fixed.] Maximize the worst-case number of fixed variables, i.e.
\[
f(\formset) = -\min_{\form\in\formset} \fixed{\form}.
\]
\item[Exp-fixed.] Maximize the expected number of fixed variables, i.e.
\[
f(\formset) = -\frac{\sum_{\form\in\formset}\numval{\form}\cdot\fixed{\form}}{\sum_{\form\in\formset}\numval{\form}}.
\]
\end{description}
\pagebreak

\begin{example}
Recall \autoref{ex:run1} and consider the following two experiments in the first step.
First, the experiment of weighing coin 1 against coin 2.
All the three outcomes are satisfiable, the number of models is
  2 for the outcome $<$, 2 for $>$ and 4 for $=$.
If the experiment results in $<$ or $>$, we know that the counterfeit coin
is the first or the second coin. If it results in $=$, it must be the third or the fourth coin.
Therefore, every outcome fixes two variables.

Second, the experiment of weighing coins 1 and 2 against coins 3 and 4.
As exactly one coin must be counterfeit, the outcome $=$ is impossible.
The outcomes $<$ and $>$ are symmetrical, both have 4 models and fix no variables.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{l|cc}
& 12 & 1234 \\\hline
Max-models & 4 & 4 \\
Exp-models & 3 & 4 \\
Ent-models & -1.04 & -0.69 \\
Parts & -3 & -2 \\
Min-fixed & -2 & 0 \\
Exp-fixed & -2 & 0 \\
\end{tabular}
\caption{Values of various one-step look-ahead strategies in the counterfeit \\
  coin problem with four coins on experiments 12 and 1234.}
\label{tbl:run-oslas}
\end{center}
\end{table}

\autoref{tbl:run-oslas} shows the values of the defined one-step look-ahead
  strategies on these two experiments.
In all strategies except for ``max-models'', the experiment 12 winds over 1234
  and is selected as the first experiment.
In ``max-models'', the values on 12 and 1234 are the same but the experiment
  12 is still selected because it is lexicographically smaller. \eqed
\end{example}


